{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Artigo 4 - This is a Disaster!","metadata":{}},{"cell_type":"markdown","source":"## Objetivo\n\nPara esse quarto artigo, o objetivo é por em prática os conhecimentos da lição 4 que se trata de processamento de linguagem natural, tendo como objetivo riar uma nova inteligência artificial capaz de fazer um processamento de linguagem natural nso dados, que no caso se tem como objetivo identificar se um determinado tweet se trata de um desastre natural ou não.\nfazer um modelo de processamento de linguagem natural, que seja capaz de indeitificar através de tweets se realmente é um tweet de um desastre natural ou não. Além da criação da IA em si, no final ela será disponibilizada na plataforma Hugging Face.","metadata":{}},{"cell_type":"markdown","source":"## Motivação\n\nPor questões de segurança da população, é de interesse que se saiba se o tweet se trata de um desastre natural ou não, uma vez que se essa identificação ocorrer, se saberá que determinado lugar pode ser classificado como perigoso naquele momento.","metadata":{}},{"cell_type":"markdown","source":"## Requisitos\n\nPara que se possa realizar esse modelo, é necessário os dados, que serão retirados da competição [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/competitions/nlp-getting-started/overview) e das bibliotecas que serão usadas para sua manipulação e criação e treinamento do modelo, que pode ser encontradas na célula 1.","metadata":{}},{"cell_type":"code","source":"# Não serão instaladas poís o Kaggle já as possui, nesa caso serão apenas importadas\n\nimport pandas as pd #Usado para a manipulação do arquivo com os dados\nimport numpy as np  #Usado para a manipulação matemática que irá ajudar as métricas\nfrom datasets import Dataset, DatasetDict #Usado para usar o formato de data frame próprio do Hugging Face\nfrom transformers import AutoModelForSequenceClassification,AutoTokenizer #Usado para poder baixar o tokenizador e criaçào do modelo\nfrom transformers import TrainingArguments,Trainer #Usado para criar os argumentos de treino e realizar o treinamento","metadata":{"execution":{"iopub.status.busy":"2023-10-26T01:25:23.191628Z","iopub.execute_input":"2023-10-26T01:25:23.192302Z","iopub.status.idle":"2023-10-26T01:25:29.966725Z","shell.execute_reply.started":"2023-10-26T01:25:23.192264Z","shell.execute_reply":"2023-10-26T01:25:29.965817Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Dados\n\nAgora pode se começar a trazer os dados para poder se analisar o que se tem, como demostrado ná célula 2.\n\nPara não ter que usar a API do Kagle e configurar uma chave eles serão baixados manualmente e colcoados na base de dados desse arquivo.","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/disaster'\ntrain_csv = f'{path}/train.csv'\ntest_csv = f'{path}/test.csv'","metadata":{"execution":{"iopub.status.busy":"2023-10-26T01:25:29.968174Z","iopub.execute_input":"2023-10-26T01:25:29.968462Z","iopub.status.idle":"2023-10-26T01:25:29.973222Z","shell.execute_reply.started":"2023-10-26T01:25:29.968437Z","shell.execute_reply":"2023-10-26T01:25:29.972113Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Agora com as caminhos definidos pode-se usar o pandas para realizar a leitura do csv de treino e visualizar o que ele traz, como também será transformado os valores inteiros em float por que esse é o padrão que será necessário mais para frente no trinamento do modelo, como demostrado na célula 3.","metadata":{}},{"cell_type":"code","source":"train= pd.read_csv(train_csv)\ntrain = train.astype({'target':'float'})\ntrain","metadata":{"execution":{"iopub.status.busy":"2023-10-26T01:25:29.974327Z","iopub.execute_input":"2023-10-26T01:25:29.974613Z","iopub.status.idle":"2023-10-26T01:25:30.022396Z","shell.execute_reply.started":"2023-10-26T01:25:29.974589Z","shell.execute_reply":"2023-10-26T01:25:30.021489Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"         id keyword location  \\\n0         1     NaN      NaN   \n1         4     NaN      NaN   \n2         5     NaN      NaN   \n3         6     NaN      NaN   \n4         7     NaN      NaN   \n...     ...     ...      ...   \n7608  10869     NaN      NaN   \n7609  10870     NaN      NaN   \n7610  10871     NaN      NaN   \n7611  10872     NaN      NaN   \n7612  10873     NaN      NaN   \n\n                                                   text  target  \n0     Our Deeds are the Reason of this #earthquake M...     1.0  \n1                Forest fire near La Ronge Sask. Canada     1.0  \n2     All residents asked to 'shelter in place' are ...     1.0  \n3     13,000 people receive #wildfires evacuation or...     1.0  \n4     Just got sent this photo from Ruby #Alaska as ...     1.0  \n...                                                 ...     ...  \n7608  Two giant cranes holding a bridge collapse int...     1.0  \n7609  @aria_ahrary @TheTawniest The out of control w...     1.0  \n7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...     1.0  \n7611  Police investigating after an e-bike collided ...     1.0  \n7612  The Latest: More Homes Razed by Northern Calif...     1.0  \n\n[7613 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7608</th>\n      <td>10869</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Two giant cranes holding a bridge collapse int...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>7609</th>\n      <td>10870</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>7610</th>\n      <td>10871</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>7611</th>\n      <td>10872</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Police investigating after an e-bike collided ...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>7612</th>\n      <td>10873</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>The Latest: More Homes Razed by Northern Calif...</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>7613 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Uma das primerias coisas que podem ser observadas é que o dataframe possui uma boa quantidade de dados, quase chegando a 10 mil linhas, ou seja, isso indica que possa haver uma grande variedade interessante para os treinos.\n\nPara uma visualização ainda melhor, na célula 4 pode se observar a forma como esses dados estão dispostos no conjunto.","metadata":{}},{"cell_type":"code","source":"train.describe(include='object')","metadata":{"execution":{"iopub.status.busy":"2023-10-26T01:25:30.025473Z","iopub.execute_input":"2023-10-26T01:25:30.025757Z","iopub.status.idle":"2023-10-26T01:25:30.050963Z","shell.execute_reply.started":"2023-10-26T01:25:30.025734Z","shell.execute_reply":"2023-10-26T01:25:30.049983Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"           keyword location                                               text\ncount         7552     5080                                               7613\nunique         221     3341                                               7503\ntop     fatalities      USA  11-Year-Old Boy Charged With Manslaughter of T...\nfreq            45      104                                                 10","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>7552</td>\n      <td>5080</td>\n      <td>7613</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>221</td>\n      <td>3341</td>\n      <td>7503</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>fatalities</td>\n      <td>USA</td>\n      <td>11-Year-Old Boy Charged With Manslaughter of T...</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>45</td>\n      <td>104</td>\n      <td>10</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Pode se observar que existem bastante dados repetidos, principalmente nas palavras chaves, mas isso já era esperado.\n\nAgora com os dados visualizados, pode-se ver que a coluna target já está em formato numérico, coisa que será necessário, porém existem colunas com o valor NaN (Not a Number) que talvez tenham a necessidade de serem tradas posteriormente.\n\nMas em um primeiro momento, será criado apenas uma nova coluna chamada input que armazenará apenas o text, que é a principal fonte de informação para decidir se algo foi um desastre ou não, que pode ser observado na célula 5.","metadata":{}},{"cell_type":"code","source":"train['input'] = 'TEXT: ' + train['text']\n\ntrain.input.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-26T01:25:30.052209Z","iopub.execute_input":"2023-10-26T01:25:30.052627Z","iopub.status.idle":"2023-10-26T01:25:30.063070Z","shell.execute_reply.started":"2023-10-26T01:25:30.052575Z","shell.execute_reply":"2023-10-26T01:25:30.062012Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"0    TEXT: Our Deeds are the Reason of this #earthq...\n1         TEXT: Forest fire near La Ronge Sask. Canada\n2    TEXT: All residents asked to 'shelter in place...\n3    TEXT: 13,000 people receive #wildfires evacuat...\n4    TEXT: Just got sent this photo from Ruby #Alas...\nName: input, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"Com isso já etá pronto para começar a trata-lo para poder ser utilizado para o treinamento do modelo.","metadata":{}},{"cell_type":"markdown","source":"### Preparando para o Modelo\n\nPara essa parte será feito a seleção do modelo a ser usado e com isso será feito a tokenização e a  referente a como o modelo trabalha, para que ele possa interpretar os dados.\n\nO primeiro é transformar em um dataset do hugging face, que pode ser obervado na célula 6.","metadata":{}},{"cell_type":"code","source":"train_ds = Dataset.from_pandas(train)\n\ntrain_ds","metadata":{"execution":{"iopub.status.busy":"2023-10-26T01:25:30.064241Z","iopub.execute_input":"2023-10-26T01:25:30.064594Z","iopub.status.idle":"2023-10-26T01:25:30.084044Z","shell.execute_reply.started":"2023-10-26T01:25:30.064567Z","shell.execute_reply":"2023-10-26T01:25:30.083112Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['id', 'keyword', 'location', 'text', 'target', 'input'],\n    num_rows: 7613\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"Agora chegou a hora de selecionar o modelo, para isso, por questões de conveniência, será usado o mesmo modelo apresentada na lição 4 d fastai, porém podem ser encontrados outros modelos no Hugging Face.\n\nEsse processo pode ser obervado na célula 7.","metadata":{}},{"cell_type":"code","source":"model_nm= 'microsoft/deberta-v3-small'","metadata":{"execution":{"iopub.status.busy":"2023-10-26T01:25:30.085057Z","iopub.execute_input":"2023-10-26T01:25:30.085336Z","iopub.status.idle":"2023-10-26T01:25:30.089720Z","shell.execute_reply.started":"2023-10-26T01:25:30.085312Z","shell.execute_reply":"2023-10-26T01:25:30.088568Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Agora será baixado os detalhes do modelo, como sua tokenização, e aplicado em uma linha do dataset para oder observar seu funcionamento, que pode ser obervado na célula 8.","metadata":{}},{"cell_type":"code","source":"tokz = AutoTokenizer.from_pretrained(model_nm)\n\ntokz.tokenize(\"Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\")","metadata":{"execution":{"iopub.status.busy":"2023-10-26T01:25:30.091144Z","iopub.execute_input":"2023-10-26T01:25:30.091459Z","iopub.status.idle":"2023-10-26T01:25:31.483272Z","shell.execute_reply.started":"2023-10-26T01:25:30.091434Z","shell.execute_reply":"2023-10-26T01:25:31.482295Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['▁Our',\n '▁Deeds',\n '▁are',\n '▁the',\n '▁Reason',\n '▁of',\n '▁this',\n '▁#',\n 'earthquake',\n '▁May',\n '▁ALL',\n 'AH',\n '▁Forgive',\n '▁us',\n '▁all']"},"metadata":{}}]},{"cell_type":"markdown","source":"Pode se observar que esse tokenizador está separando as palavras por '_'.\n\nDesse modo, o texto do dataset pode ser representado no vocabulário do modelo que se deseja usar. Agora deve ser feito a numeralização de toda a coluna input, para que seja atribuido um idex para cada palavra para que se possa repreesentar elas de forma numérica e se passar para o modelo, que pode ser obsrevado na célula 9, através de uma função.","metadata":{}},{"cell_type":"code","source":"def toke_func(x): return tokz(x[\"input\"])\n\ntokz_train = train_ds.map(toke_func, batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-26T01:25:31.484457Z","iopub.execute_input":"2023-10-26T01:25:31.484795Z","iopub.status.idle":"2023-10-26T01:25:32.119439Z","shell.execute_reply.started":"2023-10-26T01:25:31.484767Z","shell.execute_reply":"2023-10-26T01:25:32.118485Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"063ae5bad1f64d1eaa715b8983a9ba8c"}},"metadata":{}}]},{"cell_type":"markdown","source":"Agora pode se visualizar como ficaram todos os dados tokenizados e depois numeralizados ná célula 10.","metadata":{}},{"cell_type":"code","source":"linha = tokz_train[0]\nlinha['input'], linha['input_ids']","metadata":{"execution":{"iopub.status.busy":"2023-10-26T01:25:32.120699Z","iopub.execute_input":"2023-10-26T01:25:32.120998Z","iopub.status.idle":"2023-10-26T01:25:32.128017Z","shell.execute_reply.started":"2023-10-26T01:25:32.120972Z","shell.execute_reply":"2023-10-26T01:25:32.127105Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"('TEXT: Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all',\n [1,\n  54453,\n  294,\n  581,\n  65453,\n  281,\n  262,\n  18037,\n  265,\n  291,\n  953,\n  117831,\n  903,\n  4924,\n  17018,\n  43632,\n  381,\n  305,\n  2])"},"metadata":{}}]},{"cell_type":"markdown","source":"Agora antes que se possa começar a definir a métrica e o treinamento do modelo, deve-se lembrar que os modelos do Hugging Face precisam que o campo que se deseja prever se chame labels para o treino, esse processo pode ser observado na célula 11.","metadata":{}},{"cell_type":"code","source":"tokz_train = tokz_train.rename_columns({'target':'labels'})","metadata":{"execution":{"iopub.status.busy":"2023-10-26T01:25:32.129212Z","iopub.execute_input":"2023-10-26T01:25:32.129627Z","iopub.status.idle":"2023-10-26T01:25:32.139099Z","shell.execute_reply.started":"2023-10-26T01:25:32.129593Z","shell.execute_reply":"2023-10-26T01:25:32.138148Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Métrica\nComo não se tem uma métrica definida na competição, vai ser usado a de Pearson que é apresentada na lição 4 do fastai, que pode ser observado na lição 12.","metadata":{}},{"cell_type":"code","source":"def corr(x,y): return np.corrcoef(x,y)[0][1]\ndef corr_d(eval_pred): return {'pearson': corr(*eval_pred)}","metadata":{"execution":{"iopub.status.busy":"2023-10-26T01:25:32.140284Z","iopub.execute_input":"2023-10-26T01:25:32.140651Z","iopub.status.idle":"2023-10-26T01:25:32.148972Z","shell.execute_reply.started":"2023-10-26T01:25:32.140616Z","shell.execute_reply":"2023-10-26T01:25:32.148162Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Divisão dos Dados\nPara a última etapa dos dados, é necessário fazer a divisão do que será utilizado para treino e do que será utilizado para validação, uma vez também que os dados de teste já estào em um csv,\n\nPara isso, deverá ocorrer uma divisão nos dados tratados para serem usados uma parte para treino e outro para validação, e por fim utilizar o dados de teste para comprovar que o modelo funciona.\n\nTodo esse processo pode ser observado na célula 13.","metadata":{}},{"cell_type":"code","source":"train_dtv = tokz_train.train_test_split(0.20, seed=42)\ntrain_dtv","metadata":{"execution":{"iopub.status.busy":"2023-10-26T01:25:32.152670Z","iopub.execute_input":"2023-10-26T01:25:32.152942Z","iopub.status.idle":"2023-10-26T01:25:32.167824Z","shell.execute_reply.started":"2023-10-26T01:25:32.152918Z","shell.execute_reply":"2023-10-26T01:25:32.166858Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'keyword', 'location', 'text', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 6090\n    })\n    test: Dataset({\n        features: ['id', 'keyword', 'location', 'text', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 1523\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Treinamento\nAgora pode se realizar o treinamento do modelo, para isso será utilizado as funções ja importadas anteriormente, sendo algumas valores como batch, epochs e learning rate foram inspiradas na lição 4 do fastai, uma vez que foi demostrado na lição que a escolha desses valores é arbritário, como pode ser observado na célula 14 e 15.","metadata":{}},{"cell_type":"code","source":"bs = 128\nepochs = 4\nlr = 8e-5","metadata":{"execution":{"iopub.status.busy":"2023-10-26T01:25:32.168825Z","iopub.execute_input":"2023-10-26T01:25:32.169089Z","iopub.status.idle":"2023-10-26T01:25:32.173439Z","shell.execute_reply.started":"2023-10-26T01:25:32.169066Z","shell.execute_reply":"2023-10-26T01:25:32.172553Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n    num_train_epochs=epochs, weight_decay=0.01, report_to='none')","metadata":{"execution":{"iopub.status.busy":"2023-10-26T01:25:32.174570Z","iopub.execute_input":"2023-10-26T01:25:32.174835Z","iopub.status.idle":"2023-10-26T01:25:32.252746Z","shell.execute_reply.started":"2023-10-26T01:25:32.174811Z","shell.execute_reply":"2023-10-26T01:25:32.251577Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Agora que os argumentos de treino estào definidos, pode-se de fato criar o modelo e traina-lo, passando o dataset de treino e validação, como poder ser observado na célula 16, e por fim realizar de fato o treinamento como mostrado na célula 17.","metadata":{}},{"cell_type":"code","source":"model_prepared = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\ntrainer = Trainer(model_prepared, args, train_dataset=train_dtv['train'], eval_dataset=train_dtv['test'],\n                  tokenizer=tokz, compute_metrics=corr_d)","metadata":{"execution":{"iopub.status.busy":"2023-10-26T01:25:32.254250Z","iopub.execute_input":"2023-10-26T01:25:32.254803Z","iopub.status.idle":"2023-10-26T01:25:41.558450Z","shell.execute_reply.started":"2023-10-26T01:25:32.254764Z","shell.execute_reply":"2023-10-26T01:25:41.557557Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/286M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e53df66545c24affb12e5959618d8ce8"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-10-26T01:25:41.559583Z","iopub.execute_input":"2023-10-26T01:25:41.559880Z","iopub.status.idle":"2023-10-26T01:27:36.981823Z","shell.execute_reply.started":"2023-10-26T01:25:41.559855Z","shell.execute_reply":"2023-10-26T01:27:36.980725Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [96/96 01:48, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Pearson</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.125121</td>\n      <td>0.700062</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.121053</td>\n      <td>0.714383</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.125702</td>\n      <td>0.710924</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.133350</td>\n      <td>0.701830</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=96, training_loss=0.13481924931208292, metrics={'train_runtime': 115.1558, 'train_samples_per_second': 211.54, 'train_steps_per_second': 0.834, 'total_flos': 447729660395100.0, 'train_loss': 0.13481924931208292, 'epoch': 4.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Preparando o Teste\nAgora para se usar o modelo treinado nos dados de teste é necessário fazer a mesma preparação com o csv de teste, que pode ser observado na célula 18.","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(test_csv)\ntest['input'] = 'TEXT: ' + test['text']\ntest_ds = Dataset.from_pandas(test)\ntest_ds_tk = test_ds.map(toke_func, batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-26T01:27:36.983137Z","iopub.execute_input":"2023-10-26T01:27:36.984040Z","iopub.status.idle":"2023-10-26T01:27:37.289405Z","shell.execute_reply.started":"2023-10-26T01:27:36.984003Z","shell.execute_reply":"2023-10-26T01:27:37.288265Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2b9ffa9907941378680dceab18dd0fd"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Predição\nAgora pode-se usar a função para a predição dos dados de teste, que pode ser encontrada ná célula 19.","metadata":{}},{"cell_type":"code","source":"preds = trainer.predict(test_ds_tk).predictions.astype(int)\npreds[:15]","metadata":{"execution":{"iopub.status.busy":"2023-10-26T01:27:37.290621Z","iopub.execute_input":"2023-10-26T01:27:37.290912Z","iopub.status.idle":"2023-10-26T01:27:42.406923Z","shell.execute_reply.started":"2023-10-26T01:27:37.290887Z","shell.execute_reply":"2023-10-26T01:27:42.405920Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"array([[1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0]])"},"metadata":{}}]},{"cell_type":"markdown","source":"## Deploy\nAgora com o modelo pronto, pode se fazer sua exportação e coloca-lo no Hugging Face, onde foi feito um processo de se fazer upload através de alguns comandos, em um espaço particular e linkado com o espaço da disciplina.\n\nO deploy pode ser encontrado em [Disaster](https://huggingface.co/spaces/fastaioncampus/Disaster)","metadata":{}}]}
